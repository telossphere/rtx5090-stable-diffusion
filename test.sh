#!/bin/bash

# RTX 5090 Stable Diffusion - Test Script
# Tests the deployment and GPU compatibility

set -e

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${BLUE}ğŸ§ª Testing RTX 5090 Stable Diffusion deployment...${NC}"

# Test 1: Check if container is running
echo -e "${BLUE}1ï¸âƒ£  Checking container status...${NC}"
if docker ps | grep -q "stable-diffusion-rtx5090"; then
    echo -e "${GREEN}âœ… Container is running${NC}"
else
    echo -e "${RED}âŒ Container is not running${NC}"
    echo -e "${YELLOW}ğŸ’¡ Start the container: docker compose up -d${NC}"
    exit 1
fi

# Test 2: Check GPU access
echo -e "${BLUE}2ï¸âƒ£  Testing GPU access...${NC}"
if docker exec stable-diffusion-rtx5090 python -c "import torch; print(f'PyTorch {torch.__version__} with CUDA {torch.version.cuda}'); print(f'GPU: {torch.cuda.get_device_name(0)}')" 2>/dev/null; then
    echo -e "${GREEN}âœ… GPU access working${NC}"
else
    echo -e "${RED}âŒ GPU access failed${NC}"
    echo -e "${YELLOW}ğŸ’¡ Check if NVIDIA Docker runtime is properly configured${NC}"
    exit 1
fi

# Test 3: Check WebUI accessibility
echo -e "${BLUE}3ï¸âƒ£  Testing WebUI accessibility...${NC}"
if curl -s "http://localhost:7860" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ… WebUI is accessible${NC}"
else
    echo -e "${YELLOW}âš ï¸  WebUI not accessible yet (may still be starting)${NC}"
fi

# Test 4: Check models
echo -e "${BLUE}4ï¸âƒ£  Checking models...${NC}"
MODEL_COUNT=$(ls -1 ./models/stable-diffusion/*.safetensors 2>/dev/null | wc -l)
if [ "$MODEL_COUNT" -gt 0 ]; then
    echo -e "${GREEN}âœ… Found $MODEL_COUNT model(s)${NC}"
else
    echo -e "${YELLOW}âš ï¸  No models found. Run ./download-models-interactive.sh for interactive selection or ./download-models.sh to download all models${NC}"
fi

# Test 5: Check resource usage
echo -e "${BLUE}5ï¸âƒ£  Checking resource usage...${NC}"
echo -e "${BLUE}ğŸ“Š Container stats:${NC}"
docker stats --no-stream stable-diffusion-rtx5090

echo -e "${BLUE}ğŸ“Š GPU usage:${NC}"
nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader

echo -e "${GREEN}ğŸ‰ All tests completed!${NC}"
echo -e "${BLUE}ğŸŒ Access WebUI at: http://localhost:7860${NC}"
echo -e "${BLUE}ğŸ“Š Version: 1.0.2 with cuDNN v9.1 optimization${NC}"

# Troubleshooting section
echo -e "${BLUE}ğŸ”§ Troubleshooting tips:${NC}"
echo -e "  â€¢ If GPU access fails, ensure NVIDIA Docker runtime is installed:"
echo -e "    ${YELLOW}sudo apt-get install nvidia-docker2${NC}"
echo -e "    ${YELLOW}sudo systemctl restart docker${NC}"
echo -e "  â€¢ Check GPU availability: ${YELLOW}nvidia-smi${NC}"
echo -e "  â€¢ Verify Docker runtime: ${YELLOW}docker info | grep -i runtime${NC}"
echo -e "  â€¢ Test NVIDIA Docker: ${YELLOW}docker run --rm --gpus all nvidia/cuda:12.8.1-base-ubuntu22.04 nvidia-smi${NC}"
